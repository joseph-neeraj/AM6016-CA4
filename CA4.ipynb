{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this cell before you start\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CA4\n",
    "## due on 02/04/2019\n",
    "\n",
    "to submit the assignment, please do the following:\n",
    "\n",
    "- do `Cell -> All output -> Clear` to clear all your output\n",
    "- save the notebook (CA3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Boston Housing Data\n",
    "\n",
    "Consider the data in  `keras.datasets.boston_housing`. In this case, there are only about 400 training datasets, where each dataset consists of 13 input values which are characteristic for a given property. The output corresponds to the property price. The meaning of the various columns is explained in https://www.kaggle.com/c/boston-housing.\n",
    "\n",
    "In contrast to the previous examples, which were categorisation problems, this is now a regression problem. The challenge is to train a network, which is able to predict the price of the property. \n",
    "\n",
    "You will again find lots of examples on the internet, and it is okay to use inspiration as long as you provide the source. \n",
    "\n",
    "Adhere to the following rules:\n",
    "\n",
    "a) Train the network on the logarithm of the price, not on the price itself. Explain why this makes sense. \n",
    "\n",
    "b) You will find many examples, which use `sci-kit learn` or other packages, which we did not do in the course. Do not use them, and restrict yourself to methods and libraries which we covered\n",
    "\n",
    "c) Try to find a network, which has the smallest amount of trainable parameters, while still providing good predictions of the price.  Discuss, how small you can go. \n",
    "\n",
    "d) Once you have trained the network, explore the correlations which this network predicts:\n",
    "    - Which inputs have a positive price correlation? \n",
    "    - Which inputs have a negative price correlation? \n",
    "    - Which inputs have little/no influence on the price?\n",
    "    \n",
    "  Investigate this by feeding into the network some artificial data, which you obtain from the testing data by varying one of the input columns.\n",
    "  \n",
    "  \n",
    "Optional challenge (no extra points but extra insight!):\n",
    "\n",
    "Compare the results with standard regression methods, for example as in ST4060/ST4061 in case you have covered them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c11fa0c01e83508e2b3055f97bb94ad2",
     "grade": true,
     "grade_id": "cell-62578aed3a4137c9",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = keras.datasets.boston_housing.load_data()\n",
    "\n",
    "# from https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html\n",
    "var_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
    "             'DIS', 'RAD', 'TAX', 'PTRATIO', 'BLACK', 'LSTAT', 'MEDV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to print first n entries in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printFirstN(n, x=train_x, y=train_y):\n",
    "    print((\"{:8}\"*len(var_names)).format(*var_names))\n",
    "    for i in range(10):\n",
    "        print(\n",
    "            (\"{:<8.4}\"*len(x[i])).format(*x[i]),\n",
    "             \"{:<8.3}\".format(y[i])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the data\n",
    "print('Train X shape', train_x.shape)\n",
    "print('Train Y shape', train_y.shape)\n",
    "print('Test X shape', test_x.shape)\n",
    "print('Train Y shape', test_y.shape)\n",
    "print()\n",
    "\n",
    "#inspect a few elements to get an idea of the data\n",
    "printFirstN(10)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the values of each predictor are very different in scale when compared to each other.\n",
    "This may lead to difficulties in building a good enough model.\n",
    "Hence scale them uniformly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although scaling the data on a scale of 0 to 1, as in the following commented cell works,\n",
    "a better mean absolute error is obtained by centering the data on its mean and then scaling it using the \n",
    "standard deviation (as shown in https://www.kaggle.com/shanekonaung/boston-housing-price-dataset-with-keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = (train_x - train_x.min(axis=0)) / (train_x.max(axis=0) - train_x.min(axis=0))\n",
    "# test_x = (test_x - test_x.min(axis=0)) / (test_x.max(axis=0) - test_x.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling criteria from https://www.kaggle.com/shanekonaung/boston-housing-price-dataset-with-keras\n",
    "mean = train_x.mean(axis=0)\n",
    "train_x = train_x - mean\n",
    "std = train_x.std(axis=0)\n",
    "train_x = train_x/std\n",
    "\n",
    "test_x = test_x-mean\n",
    "test_x = test_x/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Train the network on the logarithm of the price, not on the price itself. Explain why this makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function we'll be using is mean_squared_error. This means that the price values that are too large or too small will have a major influence on the model. Log transformation is a way to reduce this influence.\n",
    "Hence : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.log(train_y)\n",
    "test_y = np.log(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printFirstN(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(numNeurons):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(numNeurons, activation=tf.nn.relu, input_shape=(train_x.shape[1],)))\n",
    "    model.add(keras.layers.Dense(numNeurons, activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    return model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit and inspect the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = buildModel(512)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "fit_result = model.fit(train_x, train_y, epochs=100, batch_size=1, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fit_result.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit_result.epoch, history['mean_absolute_error'], 'b', label='Training MAE')\n",
    "plt.plot(fit_result.epoch, history['val_mean_absolute_error'], 'r', label='Validation MAE')\n",
    "plt.title('Epoch vs MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot says that although the training MAE goes down almost monotonously, the test MAE remains more or less the same after about 60 epochs. This hints that the model has been overfitted. \n",
    "This is confirmed by the Epochs vs Loss plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit_result.epoch, history['loss'], 'b', label='Training loss')\n",
    "plt.plot(fit_result.epoch, history['val_loss'], 'r', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)  Try to find a network, which has the smallest amount of trainable parameters, while still providing good predictions of the price. Discuss, how small you can go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the number of neurons to find the model providing minimum mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "min_neurons = 8\n",
    "num_neurons = 512\n",
    "\n",
    "# dictionary for holding the mean_absolute_errors\n",
    "mae_dict = {}\n",
    "\n",
    "# dictionary for holding the models\n",
    "models_dict = {}\n",
    "while(num_neurons >= min_neurons):\n",
    "    \n",
    "    print(\"Fitting model for neurons = \", num_neurons)\n",
    "    model_test = buildModel(num_neurons)\n",
    "    model_test.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "    model_test.fit(train_x, train_y, epochs=100, batch_size=1)\n",
    "    \n",
    "    print(\"Summary for neurons = \", num_neurons)\n",
    "    model_test.summary()\n",
    "    \n",
    "    val_mse, val_mae = model_test.evaluate(test_x, test_y)\n",
    "    mae_dict[num_neurons] = val_mae\n",
    "    models_dict[num_neurons] = model_test\n",
    "    \n",
    "    num_neurons /= 2\n",
    "\n",
    "print(\"Mean absolute Error dictionary :\")\n",
    "print(mae_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of neurons which gives minimum mean absolute error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuronWithMinMAE = min(mae_dict, key=mae_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"It appears that {} neurons is enough for a network to have sufficient accuracy\".format(neuronWithMinMAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Once you have trained the network, explore the correlations which this network predicts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the model with minimum number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models_dict[neuronWithMinMAE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace each column in the data with its max value and make a new set of predictions on that.\n",
    "Comparing this with the original predictions can give an idea on the correlation of each variable\n",
    "with the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_predictions = model.predict(test_x)\n",
    "new_predictions = [];\n",
    "\n",
    "# iterate over the columns\n",
    "for j in range(len(var_names) - 1):\n",
    "    # Make a copy of the original so that its column can be modified\n",
    "    test_x_copy = np.copy(test_x)\n",
    "    \n",
    "    # set all values in the current column to the max value\n",
    "    for i in range(len(test_x)):\n",
    "        test_x_copy[i][j] = test_x.max(axis=0)[j];  \n",
    "        \n",
    "    # Now make the prediction again with the modified data\n",
    "    new_prediction = model.predict(test_x_copy)\n",
    "    new_predictions.append(new_prediction)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the Original Prediction and Modified prediction for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predictions = len(original_predictions)\n",
    "\n",
    "plt.figure(figsize=(40,40))\n",
    "for i in range(13):\n",
    "    plt.subplot(5,3, i + 1)\n",
    "    x = list(range(num_predictions))\n",
    "    plt.plot(x, original_predictions, label='Original Predictions')\n",
    "    plt.plot(x, new_predictions[i], label='Modified Predictions')\n",
    "    plt.title(var_names[i])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, conclusions on the correlations of each of the predictors on the price can be made as follows:\n",
    "\n",
    "* Positive Price Correlation : RM, RAD\n",
    "* Negative Price Correlation : CRIM\n",
    "* Little/No Price Correlation : All others\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
