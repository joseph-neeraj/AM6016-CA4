{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this cell before you start\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CA4\n",
    "## due on 02/04/2019\n",
    "\n",
    "to submit the assignment, please do the following:\n",
    "\n",
    "- do `Cell -> All output -> Clear` to clear all your output\n",
    "- save the notebook (CA3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Boston Housing Data\n",
    "\n",
    "Consider the data in  `keras.datasets.boston_housing`. In this case, there are only about 400 training datasets, where each dataset consists of 13 input values which are characteristic for a given property. The output corresponds to the property price. The meaning of the various columns is explained in https://www.kaggle.com/c/boston-housing.\n",
    "\n",
    "In contrast to the previous examples, which were categorisation problems, this is now a regression problem. The challenge is to train a network, which is able to predict the price of the property. \n",
    "\n",
    "You will again find lots of examples on the internet, and it is okay to use inspiration as long as you provide the source. \n",
    "\n",
    "Adhere to the following rules:\n",
    "\n",
    "a) Train the network on the logarithm of the price, not on the price itself. Explain why this makes sense. \n",
    "\n",
    "b) You will find many examples, which use `sci-kit learn` or other packages, which we did not do in the course. Do not use them, and restrict yourself to methods and libraries which we covered\n",
    "\n",
    "c) Try to find a network, which has the smallest amount of trainable parameters, while still providing good predictions of the price.  Discuss, how small you can go. \n",
    "\n",
    "d) Once you have trained the network, explore the correlations which this network predicts:\n",
    "    - Which inputs have a positive price correlation? \n",
    "    - Which inputs have a negative price correlation? \n",
    "    - Which inputs have little/no influence on the price?\n",
    "    \n",
    "  Investigate this by feeding into the network some artificial data, which you obtain from the testing data by varying one of the input columns.\n",
    "  \n",
    "  \n",
    "Optional challenge (no extra points but extra insight!):\n",
    "\n",
    "Compare the results with standard regression methods, for example as in ST4060/ST4061 in case you have covered them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c11fa0c01e83508e2b3055f97bb94ad2",
     "grade": true,
     "grade_id": "cell-62578aed3a4137c9",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = keras.datasets.boston_housing.load_data()\n",
    "\n",
    "# from https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html\n",
    "var_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
    "             'DIS', 'RAD', 'TAX', 'PTRATIO', 'BLACK', 'LSTAT', 'MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(range(1, len(train_y) + 1), train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_train_y = np.log(train_y)\n",
    "# log_test_y = np.log(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to print first n entries in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printFirstN(n, x=train_x, y=train_y):\n",
    "    print((\"{:8}\"*len(var_names)).format(*var_names))\n",
    "    for i in range(10):\n",
    "        print((\"{:<8.4}\"*len(x[i])).format(*x[i]),\n",
    "         \"{:<8.3}\".format(y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the data\n",
    "print('Train X shape', train_x.shape)\n",
    "print('Train Y shape', train_y.shape)\n",
    "print('Test X shape', test_x.shape)\n",
    "print('Train Y shape', test_y.shape)\n",
    "print()\n",
    "\n",
    "#inspect a few elements to get an idea of the data\n",
    "\n",
    "printFirstN(10)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the values of each predictor are very different in scale when compared to each other.\n",
    "This may lead to difficulties in building a good enough model.\n",
    "Hence scale them uniformly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = (train_x - train_x.min(axis=0)) / (train_x.max(axis=0) - train_x.min(axis=0))\n",
    "# test_x = (test_x - test_x.min(axis=0)) / (test_x.max(axis=0) - test_x.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling criteria from https://www.kaggle.com/shanekonaung/boston-housing-price-dataset-with-keras\n",
    "mean = train_x.mean(axis=0)\n",
    "train_x = train_x - mean\n",
    "std = train_x.std(axis=0)\n",
    "train_x = train_x/std\n",
    "\n",
    "test_x = test_x-mean\n",
    "test_x = test_x/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printFirstN(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(train_x.shape[1],)))\n",
    "model.add(keras.layers.Dense(512, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "fit_result = model.fit(train_x, train_y, epochs=100, batch_size=1, validation_data=(test_x, test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fit_result.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit_result.epoch, history['mean_absolute_error'], 'b', label='Training MAE')\n",
    "plt.plot(fit_result.epoch, history['val_mean_absolute_error'], 'r', label='Validation MAE')\n",
    "plt.title('Epoch vs MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot says that although the training MAE goes down almost monotonously, the test MAE remains more or less the same after about 60 epochs. This hints that the model has been overfitted. \n",
    "This is confirmed by the Epochs vs Loss plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit_result.epoch, history['loss'], 'b', label='Training loss')\n",
    "plt.plot(fit_result.epoch, history['val_loss'], 'r', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Once you have trained the network, explore the correlations which this network predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(model.predict(np.reshape(test_x[0], (1,test_x[0].shape[0]))))\n",
    "print(test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_predictions = model.predict(test_x)\n",
    "new_predictions = [];\n",
    "for j in range(len(var_names) - 1):\n",
    "    # Make a copy of the original so that its column can be modified\n",
    "    test_x_copy = np.copy(test_x)\n",
    "    \n",
    "    # change each column by a measure delta\n",
    "    for i in range(len(test_x)):\n",
    "        #delta = (test_x.max(axis=0)[j] - test_x[i][j]) * 0.75;\n",
    "        #test_x_copy[i][j] += delta;  \n",
    "        test_x_copy[i][j] += test_x.max(axis=0)[j];  \n",
    "        \n",
    "    # Now make the prediction again with the modified data\n",
    "    new_prediction = model.predict(test_x_copy)\n",
    "    new_predictions.append(new_prediction)\n",
    "    \n",
    "num_predictions = len(original_predictions)\n",
    "\n",
    "plt.figure(figsize=(40,40))\n",
    "for i in range(13):\n",
    "    plt.subplot(5,3, i + 1)\n",
    "    x = list(range(num_predictions))\n",
    "    plt.plot(x, original_predictions, label='Original Predictions')\n",
    "    plt.plot(x, new_predictions[i], label='Modified Predictions')\n",
    "    plt.title(var_names[i])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, comclusions on the correlations of each of the predictors on the price can be made as follows:\n",
    "\n",
    "* Positive Price Correlation : RM, RAD\n",
    "* Negative Price Correlation : CRIM\n",
    "* Little/No Price Correlation : All others\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
